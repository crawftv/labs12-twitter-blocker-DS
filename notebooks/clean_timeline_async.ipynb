{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import re\n",
    "from decouple import config\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "TWITTER_AUTH = tweepy.OAuthHandler(config('TWITTER_CONSUMER_KEY'),config('TWITTER_CONSUMER_SECRET'))\n",
    "\n",
    "TWITTER_AUTH.set_access_token(config('TWITTER_ACCESS_TOKEN'),config('TWITTER_ACCESS_TOKEN_SECRET'))\n",
    "\n",
    "TWITTER = tweepy.API(TWITTER_AUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_ACCESS_TOKEN =config('TWITTER_ACCESS_TOKEN')\n",
    "TWITTER_ACCESS_TOKEN_SECRET =config('TWITTER_ACCESS_TOKEN_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import re\n",
    "from decouple import config\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import nest_asyncio\n",
    "from itertools import zip_longest\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "def process_request(request):\n",
    "    \"\"\" Responds to a GET request with \"Hello world!\". Forbids a PUT request.\n",
    "    Args:\n",
    "        request (flask.Request): The request object.\n",
    "        <http://flask.pocoo.org/docs/1.0/api/#flask.Request>\n",
    "    Returns:\n",
    "        The response text, or any set of values that can be turned into a\n",
    "         Response object using `make_response`\n",
    "        <http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response>.\n",
    "    \"\"\"\n",
    "    from flask import abort\n",
    "\n",
    "    content_type = request.headers[\"content-type\"]\n",
    "    request_json = request.get_json(silent=True)\n",
    "    request_args = request.args\n",
    "\n",
    "    if content_type == \"application/json\":\n",
    "        request_json = request.get_json(silent=True)\n",
    "        # TWITTER_ACCESS_TOKEN check/set/error\n",
    "        if request_json and \"TWITTER_ACCESS_TOKEN\" in request_json:\n",
    "            TWITTER_ACCESS_TOKEN = request_json[\"TWITTER_ACCESS_TOKEN\"]\n",
    "        else:\n",
    "            raise ValueError(\"Missing a 'TWITTER_ACCESS_TOKEN'\")\n",
    "        # TWITTER_ACCESS_TOKEN_SECRET check/set/error\n",
    "        if request_json and \"TWITTER_ACCESS_TOKEN_SECRET\" in request_json:\n",
    "            TWITTER_ACCESS_TOKEN_SECRET = request_json[\"TWITTER_ACCESS_TOKEN_SECRET\"]\n",
    "        else:\n",
    "            raise ValueError(\"Missing a 'TWITTER_ACCESS_TOKEN_SECRET'\")\n",
    "\n",
    "        if request_json and \"num_pages\" in request_json:\n",
    "            num_pages = request_json[\"num_pages\"]\n",
    "        else:\n",
    "            num_pages = 1\n",
    "\n",
    "        # Call the function for the POST request.\n",
    "        if request.method == \"POST\":\n",
    "            establish_twitter_credentials(TWITTER_ACCESS_TOKEN,TWITTER_ACCESS_TOKEN_SECRET)\n",
    "            return execute_async_index_event_loop(num_pages)\n",
    "    else:\n",
    "        return abort(405)\n",
    "def establish_twitter_credentials(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET):\n",
    "    \"\"\"establish Twitter as as global. No need to pass it everytime.\n",
    "    \"\"\"\n",
    "    twitter_auth = tweepy.OAuthHandler(\n",
    "        config(\"TWITTER_CONSUMER_KEY\"), config(\"TWITTER_CONSUMER_SECRET\")\n",
    "    )\n",
    "    access_token = TWITTER_ACCESS_TOKEN\n",
    "    access_token_secret = TWITTER_ACCESS_TOKEN_SECRET\n",
    "    twitter_auth.set_access_token(access_token, access_token_secret)\n",
    "    global TWITTER\n",
    "    TWITTER = tweepy.API(twitter_auth)\n",
    "    \n",
    "\n",
    "def execute_async_index_event_loop(num_pages):\n",
    "    \"\"\"\n",
    "    This function does something analogous to compiling the get_data_asynchronously function,\n",
    "    Then it executes loop.\n",
    "    1. Call the get_data_function\n",
    "    2. Get the event_loop\n",
    "    3. Run the tasks (Much easier to understand in python 3.7, \"ensure_future\" was changed to \"create_task\")\n",
    "    4. Edge_list and top_interactions will be passed to the next functions\n",
    "    \"\"\"\n",
    "    final_output = []\n",
    "    mentions_timeline = get_mentions(final_output, num_pages)\n",
    "    future = asyncio.ensure_future(\n",
    "        get_index_data_asynchronous(\n",
    "            final_output,\n",
    "            num_pages,\n",
    "            mentions_timeline,\n",
    "        )\n",
    "    )\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(future)\n",
    "    final_output = {\"results\": final_output}\n",
    "    return json.dumps(final_output)\n",
    "\n",
    "def get_mentions(final_output, num_pages):\n",
    "    \"\"\"\n",
    "    Get tweets in which the user is mentioned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        home_timeline = TWITTER.mentions_timeline(\n",
    "            count=32 * num_pages, tweet_mode=\"extended\", exclude_rts=False\n",
    "        )\n",
    "        timeline = [process_tweet(full_tweet) for full_tweet in home_timeline]\n",
    "        # create list of lists where sublists = 32 for BERT model\n",
    "        z = grouper(timeline, 32)\n",
    "        timeline = [[i for i in subz if i is not None] for subz in z]\n",
    "        return timeline\n",
    "\n",
    "    except tweepy.TweepError:\n",
    "        print(\"tweepy.TweepError\")\n",
    "\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"mentions_timeline: Error: %s\" % e)\n",
    "\n",
    "\n",
    "async def get_index_data_asynchronous(final_output,num_pages,mentions_timeline):\n",
    "    \"\"\"\n",
    "    1. Establish an executor and number of workers\n",
    "    2. Establish the session\n",
    "    3. Establish the event loop\n",
    "    4. Create the tasks. Add two lists together. (because as I understand appending adds the list inside of a list.)\n",
    "        4a. tasks are created by list comprenhensions\n",
    "    5. Gather tasks.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        with requests.Session() as session:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            tasks = [\n",
    "                loop.run_in_executor(\n",
    "                    executor,\n",
    "                    clean_timeline,\n",
    "                    *(\n",
    "                        session,\n",
    "                        final_output,\n",
    "                        page,\n",
    "                    )\n",
    "                )\n",
    "                for page in range(num_pages)\n",
    "            ] + [\n",
    "                loop.run_in_executor(\n",
    "                    executor, clean_mentions, *(session, sub_timeline, final_output)\n",
    "                )\n",
    "                for sub_timeline in mentions_timeline\n",
    "            ]\n",
    "            for response in await asyncio.gather(*tasks):\n",
    "                pass\n",
    "\n",
    "def clean_timeline(session, final_output, page):\n",
    "    \"\"\"\n",
    "    1. Retrieve 32 tweets\n",
    "    2. Prepare the tweet for BERT\n",
    "    3. Execute the BERT analysis\n",
    "    4. Add the results to the final_output\n",
    "    \"\"\"   \n",
    "    try:\n",
    "        home_timeline = TWITTER.home_timeline(\n",
    "            count=32,\n",
    "            tweet_mode=\"extended\",\n",
    "            exlude_rts=False,\n",
    "            exclude_replies=False,\n",
    "            page=page,\n",
    "        )\n",
    "        home_timeline = [process_tweet(full_tweet) for full_tweet in home_timeline]\n",
    "        output = bert_request(home_timeline)\n",
    "        final_output += output\n",
    "\n",
    "    except tweepy.TweepError:\n",
    "        print(\"clean_timeline: tweepy.TweepError\")\n",
    "\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"clean_timeline: Error: %s\" % e)\n",
    "\n",
    "\n",
    "def clean_mentions(session, sub_timeline, final_output):\n",
    "    \"\"\"\n",
    "    Puts together the new BERT results with the final_output\n",
    "    \"\"\"\n",
    "    final_output += bert_request(sub_timeline)\n",
    "\n",
    "\n",
    "def process_tweet(full_tweet):\n",
    "    \"\"\"\n",
    "    Prepare the tweet for the BERT model.\n",
    "    \"\"\"\n",
    "    tweet = full_tweet.full_text\n",
    "    # strip username\n",
    "    tweet = re.sub(r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\", \"\", tweet)\n",
    "    # strip newlines and unicode characters that aren't formatted\n",
    "    tweet = re.sub(r\"\\n|&gt;|RT :\", \"\", tweet)\n",
    "    # strip twitter urls from tweets\n",
    "    tweet = re.sub(\n",
    "        r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))https://t.co/([A-Za-z0-9-_,\\']+)\", \"\", tweet\n",
    "    )\n",
    "    # Remove emojis\n",
    "    RE_EMOJI = re.compile(\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
    "    tweet = RE_EMOJI.sub(r\"\", tweet)\n",
    "    # remove whitespace\n",
    "    tweet = tweet.strip()\n",
    "    # make api request for toxicity analysis\n",
    "\n",
    "    tweet_info = {\n",
    "        \"tweet\": {\n",
    "            \"user_id\": full_tweet.user.id,\n",
    "            \"user_name\": full_tweet.user.name,\n",
    "            \"tweet\": full_tweet.full_text,\n",
    "            \"tweet_id\": full_tweet.id_str,\n",
    "        }\n",
    "    }\n",
    "    return tweet_info\n",
    "\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(fillvalue=fillvalue, *args)\n",
    "\n",
    "\n",
    "def bert_request(sub_timeline):\n",
    "    \"\"\"\n",
    "    1. Extract just the tweet from the sub_timeline\n",
    "    2. Pass the list of tweets to the BERT function.\n",
    "    3. Zip the original sub_timeline passed to the function\n",
    "    4. Put the zip object into a dictionary\n",
    "    \"\"\"\n",
    "    tweet_list = [tweet[\"tweet\"][\"tweet\"] for tweet in sub_timeline]\n",
    "    data = {\"description\": tweet_list, \"max_seq_length\": 32}\n",
    "    headers = {\"Content-type\": \"application/json\", \"cache-control\": \"no-cache\"}\n",
    "    data = json.dumps(data)\n",
    "    results = requests.post(\"http://35.222.5.199:5000/\", data=data, headers=headers).json()[\"results\"]\n",
    "    # results is a list comprehension of zipping tweet & bert_result lists\n",
    "    # and making two dictionary key,values out of each.\n",
    "    output = [\n",
    "        {\"tweet\": t[\"tweet\"], \"bert_result\": r}\n",
    "        for t, r in zip(sub_timeline, results)\n",
    "    ]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_timeline: Error: <class 'json.decoder.JSONDecodeError'>\n",
      "clean_timeline: Error: <class 'json.decoder.JSONDecodeError'>\n",
      "Wall time: 11.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"results\": [{\"tweet\": {\"user_id\": 32161686, \"user_name\": \"Nick Durbin\", \"tweet\": \"@crawfteevee @DstarDev Well, Crawford closed it down with the Monty Python reference. Sorry.\", \"tweet_id\": \"1128459504105009154\"}, \"bert_result\": {\"identity_hate\": 0.0, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0, \"toxic\": 0.0003}}, {\"tweet\": {\"user_id\": 2786431437, \"user_name\": \"Mckay Wrigley\", \"tweet\": \"@wawacat2 @bwinterrose @crawfteevee Very fascinating and beautiful process. Feels rewarding in a unique way to see these grow.\", \"tweet_id\": \"1121230220357275648\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0001, \"toxic\": 0.0002}}, {\"tweet\": {\"user_id\": 2786431437, \"user_name\": \"Mckay Wrigley\", \"tweet\": \"@bwinterrose @crawfteevee Super excited! First time grower so feeling pretty lucky I didn\\\\u2019t ruin it. So far.\", \"tweet_id\": \"1121224150666735616\"}, \"bert_result\": {\"identity_hate\": 0.0, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0, \"toxic\": 0.0003}}, {\"tweet\": {\"user_id\": 920935903, \"user_name\": \"Britton\", \"tweet\": \"@mckaywrigley Nice! Watermelon is the best. \\\\n@crawfteevee loves watermelon.\", \"tweet_id\": \"1121223239240306688\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0002, \"obscene\": 0.0002, \"severe_toxic\": 0.0, \"threat\": 0.0, \"toxic\": 0.0005}}, {\"tweet\": {\"user_id\": 2302718912, \"user_name\": \"BiggerBetterGreener\", \"tweet\": \"@crawfteevee @Austen 3x. After needed to change office i realized that in office hours, while i am in there, my huge apartmant is just wasteful empty. So i got a few desks for living room and bring my team of 6 to work at my home once a week. Other days all can stay home. Super happy with results.\", \"tweet_id\": \"1111586667477913600\"}, \"bert_result\": {\"identity_hate\": 0.0, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0, \"toxic\": 0.0003}}, {\"tweet\": {\"user_id\": 843308749012066304, \"user_name\": \"Peter Tobias\", \"tweet\": \"@crawfteevee @AustenAllred Complete agreement of all replies that granola is candy :-)\", \"tweet_id\": \"1080019968555708416\"}, \"bert_result\": {\"identity_hate\": 0.0, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0, \"threat\": 0.0, \"toxic\": 0.0005}}, {\"tweet\": {\"user_id\": 59281385, \"user_name\": \"Matthieu\", \"tweet\": \"@crawfu1 @PaulSkallas for example?\", \"tweet_id\": \"1048337767393107970\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0001, \"toxic\": 0.0002}}, {\"tweet\": {\"user_id\": 886358633646350340, \"user_name\": \"Paul (99%)\", \"tweet\": \"@crawfu1 The definition of introvert isn\\'t real considering it has all these qualifiers. So going on stage doesn\\'t matter but wearing sunglasses makes it ok. I don\\'t know.\", \"tweet_id\": \"1030194847402082305\"}, \"bert_result\": {\"identity_hate\": 0.0, \"insult\": 0.0002, \"obscene\": 0.0002, \"severe_toxic\": 0.0, \"threat\": 0.0, \"toxic\": 0.0009}}, {\"tweet\": {\"user_id\": 886987824989384704, \"user_name\": \"Francesca Pallopides\", \"tweet\": \"@brettfarrow @crawfu1 @webdevMason @dansmith_tweets I volunteered at a private school once. Most teachers there had switched over from public schools &amp; taken a pay cut for exactly those reasons. Headmaster said they were getting requests from public-school teachers all the time.\", \"tweet_id\": \"1030008165964890112\"}, \"bert_result\": {\"identity_hate\": 0.0, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0, \"toxic\": 0.0003}}, {\"tweet\": {\"user_id\": 14377610, \"user_name\": \"Brett Farrow\", \"tweet\": \"@crawfu1 @webdevMason @dansmith_tweets I don\\\\u2019t know if that\\\\u2019s the case, or if it\\\\u2019s because private school numbers are deflated by religious schools that pay less.\", \"tweet_id\": \"1029781487380512769\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0, \"toxic\": 0.0003}}, {\"tweet\": {\"user_id\": 15614600, \"user_name\": \"Matthew Pirkowski\", \"tweet\": \"@crawfu1 The reasonable position (i.e. we should establish some threshold of meat-product parity that we enforce socially) is perfectly consistent with use of medical technologies once safety is established.\\\\n\\\\nBut they stem from essentially similar science, hence the hypocrisy.\", \"tweet_id\": \"1018704885611618304\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0, \"toxic\": 0.0003}}, {\"tweet\": {\"user_id\": 15614600, \"user_name\": \"Matthew Pirkowski\", \"tweet\": \"@crawfu1 Yes, and this can likely be emulated with increasing fidelity over time. \\\\n\\\\nWe\\\\u2019re not talking about magical pixie dust, here. It\\\\u2019s a matter of matching physical properties.\\\\n\\\\nAdditionally it will be free of unnecessary hormones, antibiotics etc.\", \"tweet_id\": \"1018693575293333506\"}, \"bert_result\": {\"identity_hate\": 0.0, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0, \"threat\": 0.0, \"toxic\": 0.0003}}, {\"tweet\": {\"user_id\": 954465600159576065, \"user_name\": \"Crypto Alien Limb Phenomenon\", \"tweet\": \"@crawfu1 @MattPirkowski Actually it might given the fact that most pigs &amp; chickens raised for meat in industrial factory farms have higher levels of stress hormones, are fed antibiotics, growth hormones and other shit that lead to cancer and infectious outbreaks; also there\\\\u2019s the environmental impact.\", \"tweet_id\": \"1018542908096614400\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0, \"threat\": 0.0, \"toxic\": 0.0003}}, {\"tweet\": {\"user_id\": 15614600, \"user_name\": \"Matthew Pirkowski\", \"tweet\": \"@crawfu1 If you\\'d like to answer my questions, I\\'ll take the time to answer yours.\", \"tweet_id\": \"1017965538276884480\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0, \"toxic\": 0.0003}}, {\"tweet\": {\"user_id\": 15614600, \"user_name\": \"Matthew Pirkowski\", \"tweet\": \"@crawfu1 If indistinguishable, what is artificial about them? \\\\n\\\\nFor example, regarding function, what differentiates a diamond made by humans in a day from those made by nature over millennia?\\\\n\\\\nThe differentiation in language flows mainly from those who seek to maintain the status quo.\", \"tweet_id\": \"1017877813599145984\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0001, \"toxic\": 0.0002}}, {\"tweet\": {\"user_id\": 15614600, \"user_name\": \"Matthew Pirkowski\", \"tweet\": \"@crawfu1 Elaborate on why you think so.\", \"tweet_id\": \"1017847801219125249\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0001, \"threat\": 0.0001, \"toxic\": 0.0002}}, {\"tweet\": {\"user_id\": 95715569, \"user_name\": \"Frank Mitloehner\", \"tweet\": \"@crawfu1 By the time the steer/heifer is slaughtered, it underwent a drug whithdrawal time. At the time we eat the beef products, there is basically next to nothing left.\", \"tweet_id\": \"1014566644528472064\"}, \"bert_result\": {\"identity_hate\": 0.0001, \"insult\": 0.0001, \"obscene\": 0.0001, \"severe_toxic\": 0.0, \"threat\": 0.0, \"toxic\": 0.0004}}]}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "execute_async_index_event_loop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
