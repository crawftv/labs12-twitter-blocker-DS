{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Multi-Network Crawl/Query => Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i. Load & Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-decouple\n",
    "#!pip install trio\n",
    "import json\n",
    "import tweepy\n",
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "from decouple import config\n",
    "import trio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. Load Twitter Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running in colab, upload `.env` as `env` then run:\n",
    "#!mv env .env\n",
    "\n",
    "#Load Twitter Credentials File\n",
    "TWITTER_AUTH = tweepy.OAuthHandler(config('TWITTER_CONSUMER_KEY'),config('TWITTER_CONSUMER_SECRET'))\n",
    "TWITTER_AUTH.set_access_token(config('TWITTER_ACCESS_TOKEN'),config('TWITTER_ACCESS_TOKEN_SECRET'))\n",
    "TWITTER = tweepy.API(TWITTER_AUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Capture current TwiterAPI rate limit status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the limit before running the function, then check again after and compare. \n",
    "start_api_check = TWITTER.rate_limit_status()\n",
    "limits_alpha = json_normalize(start_api_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Function: Retrieve User Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_get_user_interactions(search, output, next_query, limit):\n",
    "    \"\"\"Crawls the targeted user's timeline and returns interactions.\n",
    "    Args:\n",
    "        `search`, string: The name of the user who's timeline to search.\n",
    "        `output`, list of tuples: paired list of search & interaction targets. \n",
    "        `next_query`, list: a list of names to search on the next run. \n",
    "        `limit`, int: A flag to indicate how many results should be returned. \n",
    "    \n",
    "    Functionality:\n",
    "        Initialize search for the specific user.\n",
    "        Get the user's tweets from their timeline. \n",
    "        Cycle through all the tweets' text and join it into a mega-string.\n",
    "        Do some standardizing and replacing.\n",
    "        Strip away everything except usernames, into a string. \n",
    "        Make a list of the counts, and take the top (X) most common people. \n",
    "        Creates a list of the `top` people. No duplicates \n",
    "        Tuple the results of the search & output together.\n",
    "        Make search inputs for the next level of crawling.\n",
    "    \n",
    "    Returns:\n",
    "        This function is async and has no return statement, rather it \n",
    "        instead updates the values of `output` and `next_query` extending\n",
    "        the lists that were passed to it as args.\n",
    "    \"\"\"\n",
    "    twitter_user = TWITTER.get_user(search)\n",
    "    tweets = twitter_user.timeline(\n",
    "                count=200,\n",
    "                exclude_replies=False,\n",
    "                include_rts=True,\n",
    "                tweet_mode='extended'\n",
    "    )\n",
    "    b = [ i.full_text for i in tweets ]\n",
    "    b = \" \".join(b)\n",
    "    b = b.lower()\n",
    "    b = b.replace(search, \"\")\n",
    "    out = re.findall(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)',b)\n",
    "    top = Counter(out).most_common(limit)\n",
    "    \n",
    "    if limit > 0:\n",
    "        interactions = []\n",
    "        for interaction_count in top:\n",
    "            interactions += ([interaction_count[0]] * interaction_count[1])\n",
    "\n",
    "        tweet_data = [(search, i) for i in interactions]\n",
    "        output.extend(tweet_data)\n",
    "\n",
    "        next_target_users = [person[0] for person in top]\n",
    "        next_query.extend(next_target_users)\n",
    "    \n",
    "    elif limit == -1: \n",
    "        \n",
    "        # Tuple the results of the search & output together.\n",
    "        tweet_data = [(search, i) for i in out]\n",
    "        output = output.extend(tweet_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Async multi-query using Trio to retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interaction_chain():\n",
    "    \"\"\"Calls our async function & loops through it for each target user.\"\"\"\n",
    "    # input settings\n",
    "    first_users = ['austen','bwinterrose','justinkhan','tommycollison']\n",
    "    first_limit = 10 # -1 is no limit, anything above 1 sets limit.\n",
    "    second_limit = 10 # -1 is no limit, anything above 1 sets limit. \n",
    "    \n",
    "    # function variables\n",
    "    data = []\n",
    "    next_users = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"----------Entering 1st level. Searching the following users:----------\")\n",
    "    print(first_users)\n",
    "    \n",
    "    # Level 1 Run. \n",
    "    loop_num = 1\n",
    "    async with trio.open_nursery() as nursery:\n",
    "        for user in first_users:\n",
    "            print(\"Loop # \", loop_num, \" . Time so far:\", time.time() - start_time)\n",
    "            loop_num +=1\n",
    "            nursery.start_soon(async_get_user_interactions, user, data, next_users, first_limit)\n",
    "    \n",
    "    print(\"----------Level 1 completed. Time so far:\", time.time() - start_time)\n",
    "    \n",
    "    # Remove Level 1 output duplicates\n",
    "    def dedupe(x):\n",
    "        return list(dict.fromkeys(x))\n",
    "    \n",
    "    next_users = dedupe(next_users)\n",
    "    print(\"Level 1 Connections found:\", len(next_users))\n",
    "    print(\"---------Now Searching the following users\")\n",
    "    display(next_users)\n",
    "    \n",
    "    # Level 2 Run.\n",
    "    loop_num = 1\n",
    "    async with trio.open_nursery() as nursery:\n",
    "        for user in next_users:\n",
    "            print(\"Loop # \", loop_num, \" . Time so far:\", time.time() - start_time)\n",
    "            loop_num +=1\n",
    "            nursery.start_soon(async_get_user_interactions, user, data, next_users, second_limit)\n",
    "    \n",
    "    print(\"Total time:\", time.time() - start_time)\n",
    "    print(\"Connections found:\", len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Call the async `Trio` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Entering 1st level. Searching the following users:----------\n",
      "['austen', 'bwinterrose', 'justinkhan', 'tommycollison']\n",
      "Loop #  1  . Time so far: 0.009862184524536133\n",
      "Loop #  2  . Time so far: 0.01004791259765625\n",
      "Loop #  3  . Time so far: 0.010246038436889648\n",
      "Loop #  4  . Time so far: 0.010425090789794922\n",
      "----------Level 1 completed. Time so far: 3.8551149368286133\n",
      "Level 1 Connections found: 27\n",
      "---------Now Searching the following users\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lambdaschool',\n",
       " 'austen',\n",
       " 'calebhicks',\n",
       " 'trevmckendrick',\n",
       " 'mitchellbwright',\n",
       " 'ouraring',\n",
       " 'ryanleehamblin',\n",
       " 'mckaywrigley',\n",
       " 'davecraige',\n",
       " 'sunjieming',\n",
       " 'honestduane',\n",
       " 'rjnance',\n",
       " 'johnnewcastle2',\n",
       " 'slouischarles',\n",
       " 'bitcoinbingbong',\n",
       " 'ryanallred',\n",
       " 'webdevmason',\n",
       " 'ohmaar1',\n",
       " 'danoherrin',\n",
       " 'kaggle',\n",
       " 'superhuman',\n",
       " 'tommycollison',\n",
       " 'paulg',\n",
       " 'mwseibel',\n",
       " 'arachnocapital2',\n",
       " 'rasbt',\n",
       " 'jason']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop #  1  . Time so far: 3.8630330562591553\n",
      "Loop #  2  . Time so far: 3.8632891178131104\n",
      "Loop #  3  . Time so far: 3.8634610176086426\n",
      "Loop #  4  . Time so far: 3.863654136657715\n",
      "Loop #  5  . Time so far: 3.8638250827789307\n",
      "Loop #  6  . Time so far: 3.8641250133514404\n",
      "Loop #  7  . Time so far: 3.865907907485962\n",
      "Loop #  8  . Time so far: 3.8661880493164062\n",
      "Loop #  9  . Time so far: 3.866259813308716\n",
      "Loop #  10  . Time so far: 3.8666670322418213\n",
      "Loop #  11  . Time so far: 3.8667140007019043\n",
      "Loop #  12  . Time so far: 3.866755962371826\n",
      "Loop #  13  . Time so far: 3.866849899291992\n",
      "Loop #  14  . Time so far: 3.8669190406799316\n",
      "Loop #  15  . Time so far: 3.8669869899749756\n",
      "Loop #  16  . Time so far: 3.8676841259002686\n",
      "Loop #  17  . Time so far: 3.86783504486084\n",
      "Loop #  18  . Time so far: 3.8679721355438232\n",
      "Loop #  19  . Time so far: 3.8682940006256104\n",
      "Loop #  20  . Time so far: 3.868691921234131\n",
      "Loop #  21  . Time so far: 3.8687920570373535\n",
      "Loop #  22  . Time so far: 3.869276762008667\n",
      "Loop #  23  . Time so far: 3.8696160316467285\n",
      "Loop #  24  . Time so far: 3.8697190284729004\n",
      "Loop #  25  . Time so far: 3.8698410987854004\n",
      "Loop #  26  . Time so far: 3.8702728748321533\n",
      "Loop #  27  . Time so far: 3.8704490661621094\n"
     ]
    }
   ],
   "source": [
    "data = trio.run(interaction_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prep the data for conversion into a `networkx MultiDiGraph` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF before Groupby\n",
    "df = pd.DataFrame(data, columns=['source_user', 'interaction_user'])\n",
    "display(df.shape)\n",
    "display(df.head(5))\n",
    "\n",
    "# Create groupby counts\n",
    "df_group = df.groupby(['source_user','interaction_user']).size().reset_index().rename(columns={0: \"count\"})\n",
    "display(df_group.head())\n",
    "display(df_group.shape)\n",
    "\n",
    "# Create \"Normalized\" interaction weights for each user's interactions. \n",
    "a = df_group.groupby('source_user')['count'].transform('sum')\n",
    "df_group['weight'] = df_group['count'].div(a)\n",
    "display(df_group.head())\n",
    "display(df_group.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert the Pandas Dataframe to Networkx Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create the graph object \n",
    "DG = nx.from_pandas_edgelist(df_group, \"source_user\", \"interaction_user\",\n",
    "                            edge_attr=['weight', \"count\"], \n",
    "                             create_using=nx.MultiGraph())\n",
    "\n",
    "# Check how many nodes are on the graph now. \n",
    "bicomponents = list(nx.biconnected_components(DG))\n",
    "len(bicomponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicomponents = list(nx.biconnected_components(DG))\n",
    "len(bicomponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Graph the Networkx graph objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "nx.draw_kamada_kawai(DG,node_size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "nx.draw_spectral(DG, node_size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query TwitterAPI and Inspect Rate Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query API \n",
    "end_api_check = TWITTER.rate_limit_status()\n",
    "limits_beta = json_normalize(end_api_check).T\n",
    "limits_beta.rename(columns = {0:'beta',}, inplace = True)\n",
    "\n",
    "# Compare the change of ALL API ENDPOINTS between limits, alpha (before run), and beta (after run.)\n",
    "limits_delta = limits_alpha.T.copy()\n",
    "limits_delta['beta'] = limits_beta['beta']\n",
    "limits_delta = limits_delta.reset_index(drop=False)\n",
    "limits_delta.rename(columns = {0:'alpha', 'index':'api_endpoint'}, inplace = True)\n",
    "limits_delta = limits_delta[['api_endpoint','alpha', 'beta']].assign(delta=limits_delta.alpha != limits_delta.beta)\n",
    "limits_delta['type'] = limits_delta.api_endpoint.str.split(pat = '.', n = 1, expand = True)[0]\n",
    "limits_delta['sub_type'] = limits_delta.api_endpoint.str.split(pat = '.', n = 2, expand = True)[1]\n",
    "limits_delta['api_path'] = limits_delta.api_endpoint.str.split(pat = '.', n = 2, expand = True)[2].str.rsplit(pat = '.', n = 1, expand = True)[0]\n",
    "limits_delta['method'] = limits_delta.api_path.str.rsplit(pat = '/', n = 1, expand = True)[1]\n",
    "limits_delta['stat'] = limits_delta.api_endpoint.str.rsplit(pat = '.', n = 1, expand = True)[1]\n",
    "limits_delta = limits_delta[['type', 'sub_type', 'api_path', 'method', 'stat', 'alpha', 'beta', 'delta']]\n",
    "\n",
    "# Display filtered df. \n",
    "limits_delta[(limits_delta['stat'].str.contains(\"reset\") == False) & (limits_delta['delta']==True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
